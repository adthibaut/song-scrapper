{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fJ_gWsRqF8ut",
        "1n5sbMrqG9VC",
        "cw24fGTrFw4t"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPKbsVwt8zXYwfn359X4ZZC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athibaut2017/web_scrapper/blob/main/Song_Scrapper_bs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projet Web scrapper\n",
        "\n",
        "L'idée est ici d'élaborer un webscrapper qui permette d'aller collecter l'ensemble des paroles des chansons du groupe Gojira, et d'effectuer une courte étude sur les mots qui y reviennent le plus fréquemment. "
      ],
      "metadata": {
        "id": "fJ_gWsRqF8ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import des librairies"
      ],
      "metadata": {
        "id": "1n5sbMrqG9VC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6F3ieO7oBsh1"
      },
      "outputs": [],
      "source": [
        "#librairie qui permet d'effectuer des requêtes HTTP\n",
        "import requests\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import yaml\n",
        "\n",
        "from lxml import etree"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Première étape : gestion des proxies"
      ],
      "metadata": {
        "id": "cw24fGTrFw4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def proxies_scrapper():\n",
        "  \"\"\"Permet de recuperer une liste de proxies gratuits et de les stocker dans un df\n",
        "  \"\"\"\n",
        "  r_proxies = requests.get('https://free-proxy-list.net/')\n",
        "  proxy_list = pd.read_html(r_proxies.text)[0]\n",
        "  \n",
        "  #format de l'adresse url\n",
        "  proxy_list['url'] = 'http://' + proxy_list['IP Address'] + ':' + proxy_list['Port'].astype(str)\n",
        "  return proxy_list\n",
        "\n",
        "df = proxies_scrapper()"
      ],
      "metadata": {
        "id": "0R80rpJTIyjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On filtre le df pour n'avoir que les adresses fr\n",
        "#df_fr = df.where(df.Code == 'FR').dropna()\n",
        "#print(df_fr)\n",
        "\n",
        "# On filtre le df pour n'obtenir que les adresses qui supportent le Https\n",
        "df_clean = df.where(df.Https == 'yes').dropna()\n",
        "\n",
        "print(df_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXJ5De41I02i",
        "outputId": "ba95055d-3814-4952-cf01-6f891d0d94aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          IP Address    Port Code        Country    Anonymity Google Https  \\\n",
            "2     146.158.19.130  8080.0   UZ     Uzbekistan  elite proxy     no   yes   \n",
            "3      202.40.177.69    80.0   BD     Bangladesh  elite proxy     no   yes   \n",
            "4        198.27.74.6  9300.0   CA         Canada  elite proxy     no   yes   \n",
            "7      112.217.162.5  3128.0   KR    South Korea    anonymous     no   yes   \n",
            "9       23.229.80.23  3129.0   US  United States  elite proxy     no   yes   \n",
            "..               ...     ...  ...            ...          ...    ...   ...   \n",
            "292  163.116.158.116  8081.0   US  United States  elite proxy     no   yes   \n",
            "293    200.119.89.19    80.0   CO       Colombia  elite proxy     no   yes   \n",
            "295   47.243.167.134  8889.0   HK      Hong Kong    anonymous     no   yes   \n",
            "297     45.32.69.105  3128.0   US  United States  elite proxy     no   yes   \n",
            "299      51.79.50.31  9300.0   CA         Canada  elite proxy     no   yes   \n",
            "\n",
            "    Last Checked                          url  \n",
            "2    25 secs ago   http://146.158.19.130:8080  \n",
            "3    25 secs ago      http://202.40.177.69:80  \n",
            "4    25 secs ago      http://198.27.74.6:9300  \n",
            "7    25 secs ago    http://112.217.162.5:3128  \n",
            "9    25 secs ago     http://23.229.80.23:3129  \n",
            "..           ...                          ...  \n",
            "292  31 mins ago  http://163.116.158.116:8081  \n",
            "293  31 mins ago      http://200.119.89.19:80  \n",
            "295  31 mins ago   http://47.243.167.134:8889  \n",
            "297  31 mins ago     http://45.32.69.105:3128  \n",
            "299  31 mins ago      http://51.79.50.31:9300  \n",
            "\n",
            "[77 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def define_headers():\n",
        "  \"\"\"Permet de definir les header à partir d'un fichier Yaml\n",
        "  \"\"\"\n",
        "  with open(\"headers.yml\") as f_headers:\n",
        "    header = yaml.safe_load(f_headers)\n",
        "  return header"
      ],
      "metadata": {
        "id": "4FbqAzUANCZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def check_proxies(df):\n",
        "  \"\"\"Permet de verifier si les proxies contenus dans un df sont valides\n",
        "  \"\"\"\n",
        "  url = 'https://httpbin.org/ip' #renvoie la requête envoyée\n",
        "  good_proxies = set()\n",
        "  headers = define_headers()\n",
        "  for proxy_url in df['url']:\n",
        "    proxies = { \"http\": proxy_url,\n",
        "                \"https\": proxy_url,\n",
        "    }\n",
        "    try:\n",
        "      r = requests.get(url, headers=headers['Firefox'], proxies=proxies, timeout=3)\n",
        "      good_proxies.add(proxy_url)\n",
        "      print('Proxy ' + proxy_url + ' ajouté à la liste des proxies valides')\n",
        "    except Exception:\n",
        "      pass\n",
        "    if len(good_proxies) >= 5:\n",
        "      break\n",
        "  return good_proxies\n",
        "\n",
        "check_proxies(df_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DraKfGhLBMb",
        "outputId": "db9d272c-8cf8-4ca8-8462-dc84b87f843a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proxy http://202.40.177.69:80 ajouté à la liste des proxies valides\n",
            "Proxy http://112.217.162.5:3128 ajouté à la liste des proxies valides\n",
            "Proxy http://46.4.242.149:3128 ajouté à la liste des proxies valides\n",
            "Proxy http://217.64.14.171:8080 ajouté à la liste des proxies valides\n",
            "Proxy http://115.144.101.200:10000 ajouté à la liste des proxies valides\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'http://112.217.162.5:3128',\n",
              " 'http://115.144.101.200:10000',\n",
              " 'http://202.40.177.69:80',\n",
              " 'http://217.64.14.171:8080',\n",
              " 'http://46.4.242.149:3128'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_header_proxies():\n",
        "\n",
        "  url='https://httpbin.org/ip'\n",
        "\n",
        "  headers = define_headers()\n",
        "  good_proxies = check_proxies(df_clean)\n",
        "  for browser, header in headers.items():\n",
        "    print(f'On utilise les {browser} headers')\n",
        "    for proxy_url in good_proxies:\n",
        "      proxies = proxies = { \n",
        "          \"http\": proxy_url,\n",
        "          \"https\": proxy_url,\n",
        "      }\n",
        "      try:\n",
        "        r = requests.get(url, headers=headers, proxies=proxies, timeout=2)\n",
        "        print(r.json())\n",
        "      except Exception:\n",
        "        print(f'Proxy {proxy_url} failed, trying an other one')\n",
        "\n",
        "rotate_header_proxies()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozvn6hctJygf",
        "outputId": "7ab7d179-c9fe-4eaf-a56f-450d7b145a26"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proxy http://112.217.162.5:3128 ajouté à la liste des proxies valides\n",
            "Proxy http://207.188.11.31:80 ajouté à la liste des proxies valides\n",
            "Proxy http://115.144.101.200:10000 ajouté à la liste des proxies valides\n",
            "Proxy http://47.243.55.21:8080 ajouté à la liste des proxies valides\n",
            "Proxy http://49.0.2.242:8090 ajouté à la liste des proxies valides\n",
            "On utilise les Chrome headers\n",
            "Proxy http://115.144.101.200:10000 failed, trying an other one\n",
            "Proxy http://207.188.11.31:80 failed, trying an other one\n",
            "Proxy http://112.217.162.5:3128 failed, trying an other one\n",
            "Proxy http://49.0.2.242:8090 failed, trying an other one\n",
            "Proxy http://47.243.55.21:8080 failed, trying an other one\n",
            "On utilise les Edge headers\n",
            "Proxy http://115.144.101.200:10000 failed, trying an other one\n",
            "Proxy http://207.188.11.31:80 failed, trying an other one\n",
            "Proxy http://112.217.162.5:3128 failed, trying an other one\n",
            "Proxy http://49.0.2.242:8090 failed, trying an other one\n",
            "Proxy http://47.243.55.21:8080 failed, trying an other one\n",
            "On utilise les Firefox headers\n",
            "Proxy http://115.144.101.200:10000 failed, trying an other one\n",
            "Proxy http://207.188.11.31:80 failed, trying an other one\n",
            "Proxy http://112.217.162.5:3128 failed, trying an other one\n",
            "Proxy http://49.0.2.242:8090 failed, trying an other one\n",
            "Proxy http://47.243.55.21:8080 failed, trying an other one\n",
            "On utilise les IE headers\n",
            "Proxy http://115.144.101.200:10000 failed, trying an other one\n",
            "Proxy http://207.188.11.31:80 failed, trying an other one\n",
            "Proxy http://112.217.162.5:3128 failed, trying an other one\n",
            "Proxy http://49.0.2.242:8090 failed, trying an other one\n",
            "Proxy http://47.243.55.21:8080 failed, trying an other one\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deuxième étape : le scrapper"
      ],
      "metadata": {
        "id": "as_ePrEUDiCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_headers():\n",
        "  \"\"\"Permet de definir les header à partir d'un fichier Yaml\n",
        "  \"\"\"\n",
        "  with open(\"headers.yml\") as f_headers:\n",
        "    header = yaml.safe_load(f_headers)\n",
        "  return header"
      ],
      "metadata": {
        "id": "QI9nVfGkDhrf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Affiche l'ensemble des morceau\n",
        "def liste_morceaux(tables):\n",
        "  for row in tables:\n",
        "    print(row.text.strip())\n",
        "\n",
        "#liste_morceaux(tables)"
      ],
      "metadata": {
        "id": "Vc1s7Kw7hqZl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_all_urls():\n",
        "\n",
        "  header = define_headers()\n",
        "  url='http://www.darklyrics.com/g/gojira.html'\n",
        "  r = requests.get(url, headers=header[\"Firefox\"])\n",
        "  #print(r.status_code)\n",
        "\n",
        "  #raw HTML content\n",
        "  #print(r.content)\n",
        "\n",
        "  #on spécifie le parser HTLM que l'on veut utiliser pour mettre en forme r.content\n",
        "  soup = BeautifulSoup(r.content, 'html5lib')\n",
        "  #print(soup.prettify)\n",
        "\n",
        "  url_songs = [] # une liste pour stocker les urls d'une chanson\n",
        "  domain_name = 'http://www.darklyrics.com' #nom de domain du site\n",
        "\n",
        "  tables = soup.find_all('div', class_=\"album\")\n",
        "  #si on veut lister les morceaux\n",
        "  #liste_morceaux(tables)\n",
        "\n",
        "  for row in tables: \n",
        "    a = row.find('a')\n",
        "    #print(a)\n",
        "    try:\n",
        "      if 'href' in a.attrs:\n",
        "        url_songs.append(domain_name + a.get('href')[2:-2])\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  return url_songs\n",
        "\n"
      ],
      "metadata": {
        "id": "-4S_i4c5C_Fy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_list = fetch_all_urls()\n",
        "url_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwSK5i9stxoN",
        "outputId": "3d3e191b-1fc6-4a33-fc00-ddf58a762438"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['http://www.darklyrics.com/lyrics/gojira/terraincognita.html',\n",
              " 'http://www.darklyrics.com/lyrics/gojira/thelink.html',\n",
              " 'http://www.darklyrics.com/lyrics/gojira/frommarstosirius.html',\n",
              " 'http://www.darklyrics.com/lyrics/gojira/thewayofallflesh.html',\n",
              " 'http://www.darklyrics.com/lyrics/gojira/lenfantsauvage.html',\n",
              " 'http://www.darklyrics.com/lyrics/gojira/magma.html',\n",
              " 'http://www.darklyrics.com/lyrics/gojira/fortitude.html',\n",
              " 'http://www.darklyrics.com/lyrics/gojira/nonalbumsongs.html']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#url='http://www.darklyrics.com/lyrics/gojira/thelink.html'\n",
        "def extract_lyrics(url):\n",
        "  header = define_headers()\n",
        "  r = requests.get(url, headers=header[\"Firefox\"])\n",
        "  soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "  #on elimine les parties du HTML dont on ne veut pas\n",
        "  soup.find('div', class_='note').decompose()\n",
        "  soup.find('div', class_='thanks').decompose()\n",
        "  for h3 in soup(\"h3\"): h3.decompose()\n",
        "  for i in soup(\"i\"): i.decompose()\n",
        "  for a in soup(\"a\"): a.decompose()\n",
        "  \n",
        "  #on extrait les lyrics (pas idéal, il faudrait plutot extraire la section voulue dès le début)\n",
        "  lyrics = soup.find('div', class_='lyrics')\n",
        "\n",
        "  #on créé une liste pour ajouter tous les éléments de chaque phrase\n",
        "  words = []\n",
        "  for sentence in lyrics.stripped_strings:\n",
        "    #on nettoie les mots un par un \n",
        "    for  word in sentence.split():\n",
        "      word.lower().strip(',.')\n",
        "    words.extend(sentence.split())\n",
        "\n",
        "  #on passe tous les mots de la liste en minuscule\n",
        "  words = [word.lower() for word in words]\n",
        "\n",
        "  #permet de relancer la fonction si elle échoue une première fois\n",
        "  if not words: return extract_lyrics(url)\n",
        "\n",
        "  return words"
      ],
      "metadata": {
        "id": "mBIhT8CJzI_u"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = extract_lyrics(url)\n",
        "words"
      ],
      "metadata": {
        "id": "j0Cv1dZ4hFXi"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_lyrics():\n",
        "  \"\"\"Permet de recuperer les paroles de tous les morceaux de Gojira\n",
        "  \"\"\"\n",
        "  url_list = fetch_all_urls()\n",
        "  lyrics = []\n",
        "\n",
        "  for url in url_list:\n",
        "    lyrics.extend(extract_lyrics(url))\n",
        "\n",
        "  return lyrics"
      ],
      "metadata": {
        "id": "5z4gN9lAOG4k"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics = get_all_lyrics()"
      ],
      "metadata": {
        "id": "8k3TgedfS1yK"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(lyrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2ATMpoXS534",
        "outputId": "f5289f9d-26fc-45c7-e934-468228b5fd1b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11130"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    }
  ]
}